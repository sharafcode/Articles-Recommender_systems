Topic Modeling

In the previous chapter we clustered texts into groups. This is a very useful tool, but 
it is not always appropriate. Clustering results in each text belonging to exactly one 
cluster. This book is about machine learning and Python. Should it be grouped with 
other Python-related works or with machine-related works? In the paper book age, 
a bookstore would need to make this decision when deciding where to stock it. In 
the Internet store age, however, the answer is that this book is both about machine 
learning and Python, and the book can be listed in both sections. We will, however, 
not list it in the food section.

In this chapter, we will learn methods that do not cluster objects, but put them into 
a small number of groups called topics. We will also learn how to derive between 
topics that are central to the text and others only that are vaguely mentioned (this 
book mentions plotting every so often, but it is not a central topic such as machine 
learning is). The subfield of machine learning that deals with these problems is  
called topic modeling.

Latent Dirichlet allocation (LDA)
LDA and LDA: unfortunately, there are two methods in machine learning with 
the initials LDA: latent Dirichlet allocation, which is a topic modeling method; and 
linear discriminant analysis, which is a classification method. They are completely 
unrelated, except for the fact that the initials LDA can refer to either. However, this 
can be confusing. Scikit-learn has a submodule, sklearn.lda, which implements 
linear discriminant analysis. At the moment, scikit-learn does not implement latent 
Dirichlet allocation.
The simplest topic model (on which all others are based) is latent Dirichlet 
allocation (LDA). The mathematical ideas behind LDA are fairly complex,  
and we will not go into the details here. 

For those who are interested and adventurous enough, a Wikipedia search will 
provide all the equations behind these algorithms  
at the following link:


However, we can understand that this is at a high level and there is a sort of fable 
which underlies these models. In this fable, there are topics that are fixed. This lacks 
clarity. Which documents?
For example, let's say we have only three topics at present:

•  Machine learning
•  Python
•  Baking

Each topic has a list of words associated with it. This book would be a mixture of the 
first two topics, perhaps 50 percent each. Therefore, when we are writing it, we pick 
half of our words from the machine learning topic and half from the Python topic. In 
this model, the order of words does not matter.

The preceding explanation is a simplification of the reality; each topic assigns a 
probability to each word so that it is possible to use the word "flour" when the  
topic is either machine learning or baking, but more probable if the topic is baking.

Of course, we do not know what the topics are. Otherwise, this would be a different 
and much simpler problem. Our task right now is to take a collection of text and 
reverse engineer this fable in order to discover what topics are out there and also 
where each document belongs.

Building a topic model
Unfortunately, scikit-learn does not support latent Dirichlet allocation. Therefore, we 
are going to use the gensim package in Python. Gensim is developed by Radim Řehůřek, 
who is a machine learning researcher and consultant in the Czech Republic. We must 
start by installing it. We can achieve this by running one of the following commands:

We are going to use an Associated Press (AP) dataset of news reports. This is a 
standard dataset, which was used in some of the initial work on topic models:

Corpus is just the preloaded list of words:

This one-step process will build a topic model. We can explore the topics in  
many ways. We can see the list of topics a document refers to by using the 
model[doc] syntax:

I elided some of the output, but the format is a list of pairs (topic_index, topic_
weight). We can see that only a few topics are used for each document. The topic 
model is a sparse model, as although there are many possible topics for each 
document, only a few of them are used. We can plot a histogram of the number  
of topics as shown in the following graph:

Sparsity means that while you may have large matrices and vectors, 
in principle, most of the values are zero (or so small that we can round 
them to zero as a good approximation). Therefore, only a few things 
are relevant at any given time.
Often problems that seem too big to solve are actually feasible because 
the data is sparse. For example, even though one webpage can link to 
any other webpage, the graph of links is actually very sparse as each 
webpage will link to a very tiny fraction of all other webpages.

In the previous graph, we can see that about 150 documents have 5 topics, while  
the majority deal with around 10 to 12 of them. No document talks about more  
than 20 topics.

To a large extent, this is a function of the parameters used, namely the alpha 
parameter. The exact meaning of alpha is a bit abstract, but bigger values for alpha 
will result in more topics per document. Alpha needs to be positive, but is typically 
very small; usually smaller than one. By default, gensim will set alpha equal to 1.0/
len (corpus), but you can set it yourself as follows:

In this case, this is a larger alpha, which should lead to more topics per document. 
We could also use a smaller value. As we can see in the combined histogram given 
next, gensim behaves as we expected:

Now we can see that many documents touch upon 20 to 25 different topics.

What are these topics? Technically, they are multinomial distributions over  
words, which mean that they give each word in the vocabulary a probability.  
Words with high probability are more associated with that topic than words  
with lower probability.

Our brains aren't very good at reasoning with probability distributions, but we  
can readily make sense of a list of words. Therefore, it is typical to summarize topics 
with a the list of the most highly weighted words. Here are the first ten topics:

•  dress military soviet president new state capt carlucci states leader  

stance government

•  koch zambia lusaka one-party orange kochs party i government mayor  

new political

•  human turkey rights abuses royal thompson threats new state wrote  

garden president

•  bill employees experiments levin taxation federal measure legislation  

senate president whistleblowers sponsor

•  ohio july drought jesus disaster percent hartford mississippi crops northern 

valley virginia

•  united percent billion year president world years states people i bush news
•  b hughes affidavit states united ounces squarefoot care delaying charged 

unrealistic bush

•  yeutter dukakis bush convention farm subsidies uruguay percent secretary 

general i told

•  Kashmir government people srinagar india dumps city two jammu-kashmir 

group moslem pakistan

•  workers vietnamese irish wage immigrants percent bargaining last island 

police hutton I


Although daunting at first glance, we can clearly see that the topics are not just 
random words, but are connected. We can also see that these topics refer to older 
news items, from when the Soviet union still existed and Gorbachev was its Secretary 
General. We can also represent the topics as word clouds, making more likely words 
larger For example, this is the visualization of a topic, which deals with the Middle 
East and politics:

We can also see that some of the words should perhaps be removed (for example, 
the word I) as they are not so informative (stop words). In topic modeling, it is 
important to filter out stop words, as otherwise you might end up with a topic 
consisting entirely of stop words, which is not very informative. We may also wish 
to preprocess the text to stems in order to normalize plurals and verb forms. This 
process was covered in the previous chapter, and you can refer to it for details. If you 
are interested, you can download the code from the companion website of the book 
and try all these variations to draw different pictures.

Building a word cloud like the one in the previous screenshot can 
be done with several different pieces of software. For the previous 
graphic, I used the online tool wordle (http://www.wordle.net), 
which generates particularly attractive images. Since I only had a 
few examples, I copy and pasted the list of words manually, but it is 
possible to use it as a web service and call it directly from Python.

Comparing similarity in topic space
Topics can be useful on their own to build small vignettes with words that are in the 
previous screenshot. These visualizations could be used to navigate a large collection 
of documents and, in fact, they have been used in just this way.

However, topics are often just an intermediate tool to another end. Now that we 
have an estimate for each document about how much of that document comes from 
each topic, we can compare the documents in topic space. This simply means that 
instead of comparing word per word, we say that two documents are similar if they 
talk about the same topics.

This can be very powerful, as two text documents that share a few words may 
actually refer to the same topic. They may just refer to it using different constructions 
(for example, one may say the President of the United States while the other will use 
the name Barack Obama).

Topic models are useful on their own to build visualizations and 
explore data. They are also very useful as an intermediate step in 
many other tasks.

At this point, we can redo the exercise we performed in the previous chapter and 
look for the most similar post, but by using the topics. Whereas previously we 
compared two documents by comparing their word vectors, we can now compare 
two documents by comparing their topic vectors.

For this, we are going to project the documents to the topic space. That is, we want 
to have a vector of topics that summarizes the document. Since the number of topics 
(100) is smaller than the number of possible words, we have reduced dimensionality. 
How to perform these types of dimensionality reduction in general is an important 
task in itself, and we have a chapter entirely devoted to this task. One additional 
computational advantage is that it is much faster to compare 100 vectors of topic 
weights than vectors of the size of the vocabulary (which will contain thousands  
of terms).

Using gensim, we saw before how to compute the topics corresponding to all 
documents in the corpus:

We will store all these topic counts in NumPy arrays and compute all  
pairwise distances:

Now, dense is a matrix of topics. We can use the pdist function in SciPy to compute 
all pairwise distances. That is, with a single function call, we compute all the values 
of sum((dense[ti] – dense[tj])**2):

Now we employ one last little trick; we set the diagonal elements of the distance 
matrix to a high value (it just needs to be larger than the other values in the matrix):

The previous code would not work if we had not set the diagonal 
elements to a large value; the function would always return the same 
element as it is almost similar to itself (except in the weird case where 
two elements have exactly the same topic distribution, which is very 
rare unless they are exactly the same).

For example, here is the second document in the collection (the first document is 
very uninteresting, as the system returns a post stating that it is the most similar):


We received a post by the same author discussing medications.

Modeling the whole of Wikipedia
While the initial LDA implementations could be slow, modern systems can work 
with very large collections of data. Following the documentation of gensim, we are 
going to build a topic model for the whole of the English language Wikipedia. This 
takes hours, but can be done even with a machine that is not too powerful. With a 
cluster of machines, we could make it go much faster, but we will look at that sort of 
processing in a later chapter.

First we download the whole Wikipedia dump from http://dumps.wikimedia.
org. This is a large file (currently just over 9 GB), so it may take a while, unless your 
Internet connection is very fast. Then, we will index it with a gensim tool:

python -m gensim.scripts.make_wiki enwiki-latest-pages-articles.xml.bz2 
wiki_en_output

Run the previous command on the command line, not on the Python shell. After a 
few hours, the indexing will be finished. Finally, we can build the final topic model. 
This step looks exactly like what we did for the small AP dataset. We first import a 
few packages:
Now, we load the data that has been preprocessed:

Finally, we build the LDA model as before:

This will again take a couple of hours (you will see the progress on your console, 
which can give you an indication of how long you still have to wait). Once it is done, 
you can save it to a file so you don't have to redo it all the time:

If you exit your session and come back later, you can load the model again with:


Let us explore some topics:


We can see that this is still a sparse model even if we have many more documents 
than before (over 4 million as we are writing this):

So, the average document mentions 6.5 topics and 93 percent of them mention 10  
or fewer.

If you have not seen the idiom before, it may be odd to take the mean 
of a comparison, but it is a direct way to compute a fraction.
np.mean(lens <= 10) is taking the mean of an array of Booleans. 
The Booleans get interpreted as 0s and 1s in a numeric context. 
Therefore, the result is a number between 0 and 1, which is the fraction 
of ones. In this case, it is the fraction of elements of lens, which are 
less than or equal to 10.


We can also ask what the most talked about topic in Wikipedia is. We first collect 
some statistics on topic usage:

Using the same tool as before to build up visualization, we can see that the most 
talked about topic is fiction and stories, both as books and movies. For variety, we 
chose a different color scheme. A full 25 percent of Wikipedia pages are partially 
related to this topic (or alternatively, 5 percent of the words come from this topic):

These plots and numbers were obtained when the book was being 
written in early 2013. As Wikipedia keeps changing, your results will 
be different. We expect that the trends will be similar, but the details 
may vary. Particularly, the least relevant topic is subject to change, 
while a topic similar to the previous topic is likely to be still high on the 
list (even if not as the most important).

Alternatively, we can look at the least talked about topic:

The least talked about are the former French colonies in Central Africa. Just 1.5 
percent of documents touch upon it, and it represents 0.08 percent of the words. 
Probably if we had performed this exercise using the French Wikipedia, we would  
have obtained a very different result.

Choosing the number of topics
So far, we have used a fixed number of topics, which is 100. This was purely an 
arbitrary number; we could have just as well done 20 or 200 topics. Fortunately, 
for many users, this number does not really matter. If you are going to only use the 
topics as an intermediate step as we did previously, the final behavior of the system 
is rarely very sensitive to the exact number of topics. This means that as long as 
you use enough topics, whether you use 100 topics or 200, the recommendations 
that result from the process will not be very different. One hundred is often a good 
number (while 20 is too few for a general collection of text documents). The same 
is true of setting the alpha (α) value. While playing around with it can change the 
topics, the final results are again robust against this change.

Topic modeling is often an end towards a goal. In that case, it is not 
always important exactly which parameters you choose. Different 
numbers of topics or values for parameters such as alpha will result in 
systems whose end results are almost identical.

If you are going to explore the topics yourself or build a visualization tool, you 
should probably try a few values and see which gives you the most useful or most 
appealing results.

However, there are a few methods that will automatically determine the number of 
topics for you depending on the dataset. One popular model is called the hierarchical 
Dirichlet process. Again, the full mathematical model behind it is complex and beyond 
the scope of this book, but the fable we can tell is that instead of having the topics be 
fixed a priori and our task being to reverse engineer the data to get them back, the 
topics themselves were generated along with the data. Whenever the writer was going 
to start a new document, he had the option of using the topics that already existed or 
creating a completely new one.

This means that the more documents we have, the more topics we will end up with. 
This is one of those statements that is unintuitive at first, but makes perfect sense upon 
reflection. We are learning topics, and the more examples we have, the more we can 
break them up. If we only have a few examples of news articles, then sports will be a 
topic. However, as we have more, we start to break it up into the individual modalities 
such as Hockey, Soccer, and so on. As we have even more data, we can start to tell 
nuances apart articles about individual teams and even individual players. The same 
is true for people. In a group of many different backgrounds, with a few "computer 
people", you might put them together; in a slightly larger group, you would have 
separate gatherings for programmers and systems managers. In the real world, we 
even have different gatherings for Python and Ruby programmers.

One of the methods for automatically determining the number of topics is called 
the hierarchical Dirichlet process (HDP), and it is available in gensim. Using it is 
trivial. Taking the previous code for LDA, we just need to replace the call to gensim.
models.ldamodel.LdaModel with a call to the HdpModel constructor as follows:

That's it (except it takes a bit longer to compute—there are no free lunches). Now,  
we can use this model as much as we used the LDA model, except that we did not 
need to specify the number of topics.

Summary
In this chapter, we discussed a more advanced form of grouping documents, which is 
more flexible than simple clustering as we allow each document to be present in more 
than one group. We explored the basic LDA model using a new package, gensim, but 
were able to integrate it easily into the standard Python scientific ecosystem.
