{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Features for Authorship Attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Lexical , Punctuation , Bag of words , Syntactic)  Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import glob\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = r\"Chapters/\"\n",
    "files = sorted(glob.glob(os.path.join(data_folder, \"Chapter*.txt\")))\n",
    "chapters = []\n",
    "for fn in files:\n",
    "    with open(fn) as f:\n",
    "        chapters.append(f.read().replace('\\n', ' '))\n",
    "all_text = ' '.join(chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create feature vectors\n",
    "num_chapters = len(chapters)\n",
    "fvs_lexical = np.zeros((len(chapters), 3), np.float64)\n",
    "fvs_punct = np.zeros((len(chapters), 3), np.float64)\n",
    "for e, ch_text in enumerate(chapters):\n",
    "    # note: the nltk.word_tokenize includes punctuation\n",
    "    tokens = nltk.word_tokenize(ch_text.lower())\n",
    "    words = word_tokenizer.tokenize(ch_text.lower())\n",
    "    sentences = sentence_tokenizer.tokenize(ch_text)\n",
    "    vocab = set(words)\n",
    "    words_per_sentence = np.array([len(word_tokenizer.tokenize(s))\n",
    "                                   for s in sentences])\n",
    " \n",
    "    # average number of words per sentence\n",
    "    fvs_lexical[e, 0] = words_per_sentence.mean()\n",
    "    # sentence length variation\n",
    "    fvs_lexical[e, 1] = words_per_sentence.std()\n",
    "    # Lexical diversity\n",
    "    fvs_lexical[e, 2] = len(vocab) / float(len(words))\n",
    " \n",
    "    # Commas per sentence\n",
    "    fvs_punct[e, 0] = tokens.count(',') / float(len(sentences))\n",
    "    # Semicolons per sentence\n",
    "    fvs_punct[e, 1] = tokens.count(';') / float(len(sentences))\n",
    "    # Colons per sentence\n",
    "    fvs_punct[e, 2] = tokens.count(':') / float(len(sentences))\n",
    "\n",
    "#apply whitening to decorrelate the features\n",
    "fvs_lexical = whiten(fvs_lexical)\n",
    "fvs_punct = whiten(fvs_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get most common words in the whole book\n",
    "NUM_TOP_WORDS = 10\n",
    "all_tokens = nltk.word_tokenize(all_text)\n",
    "fdist = nltk.FreqDist(all_tokens)\n",
    "vocab = fdist.keys()[:NUM_TOP_WORDS]\n",
    " \n",
    "# use sklearn to create the bag for words feature vector for each chapter\n",
    "vectorizer = CountVectorizer(vocabulary=vocab, tokenizer=nltk.word_tokenize)\n",
    "fvs_bow = vectorizer.fit_transform(chapters).toarray().astype(np.float64)\n",
    " \n",
    "# normalise by dividing each row by its Euclidean norm\n",
    "fvs_bow /= np.c_[np.apply_along_axis(np.linalg.norm, 1, fvs_bow)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get part of speech for each token in each chapter\n",
    "def token_to_pos(ch):\n",
    "    tokens = nltk.word_tokenize(ch)\n",
    "    return [p[1] for p in nltk.pos_tag(tokens)]\n",
    "chapters_pos = [token_to_pos(ch) for ch in chapters]\n",
    " \n",
    "# count frequencies for common POS types\n",
    "pos_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "fvs_syntax = np.array([[ch.count(pos) for pos in pos_list]\n",
    "                       for ch in chapters_pos]).astype(np.float64)\n",
    " \n",
    "# normalise by dividing each row by number of tokens in the chapter\n",
    "fvs_syntax /= np.c_[np.array([len(ch) for ch in chapters_pos])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PredictAuthors(fvs):\n",
    "    km = KMeans(n_clusters=2, init='k-means++', n_init=10, verbose=0)\n",
    "    km.fit(fvs)\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For the feature 1\n",
      "['Luis P.', 'W.Richert', 'Luis P.', 'W.Richert', 'Luis P.', 'Luis P.']\n",
      "\n",
      "\n",
      "For the feature 2\n",
      "['W.Richert', 'W.Richert', 'W.Richert', 'Luis P.', 'W.Richert', 'Luis P.']\n",
      "\n",
      "\n",
      "For the feature 3\n",
      "['W.Richert', 'W.Richert', 'W.Richert', 'W.Richert', 'W.Richert', 'Luis P.']\n",
      "\n",
      "\n",
      "For the feature 4\n",
      "['W.Richert', 'Luis P.', 'W.Richert', 'Luis P.', 'W.Richert', 'W.Richert']\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for fvs in [fvs_bow, fvs_lexical, fvs_punct, fvs_syntax]:\n",
    "    i+=1\n",
    "    predictions= PredictAuthors(fvs).labels_\n",
    "    print \"\\n\\nFor the feature \" + str(i)\n",
    "    Authors_predictions = []\n",
    "    for j in range(len(predictions)):\n",
    "        if predictions[j]== 0:\n",
    "            Authors_predictions.append(\"W.Richert\")\n",
    "        else:\n",
    "            Authors_predictions.append(\"Luis P.\")\n",
    "    print Authors_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
