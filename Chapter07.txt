Classification II – Sentiment 
Analysis

For companies, it is vital to closely monitor the public reception of key events such 
as product launches or press releases. With real-time access and easy accessibility of 
user-generated content on Twitter, it is now possible to do sentiment classification 
of tweets. Sometimes also called opinion mining, it is an active field of research in 
which several companies are already selling their products. As this shows that a 
market obviously exists, we have motivation to use our classification muscles built  
in the previous chapter to build our own home-grown sentiment classifier.

Sketching our roadmap
Sentiment analysis of tweets is particularly hard because of Twitter's size limitation 
of 140 characters. This leads to a special syntax, creative abbreviations, and seldom 
well-formed sentences. The typical approach of analyzing sentences, aggregating 
their sentiment information per paragraph and then calculating the overall sentiment 
of a document, therefore, does not work here.

Clearly, we will not try to build a state-of-the-art sentiment classifier. Instead, we 
want to:

•  Use this scenario as a vehicle to introduce yet another classification 

algorithm: Naive Bayes

•  Explain how Part Of Speech (POS) tagging works and how it can help us
•  Show some more tricks from the scikit-learn toolbox that come in handy  

from time to time

Fetching the Twitter data
Naturally, we need tweets and their corresponding labels that tell us whether a  
tweet contains positive, negative, or neutral sentiment. In this chapter, we will use 
the corpus from Niek Sanders, who has done an awesome job of manually labeling 
more than 5000 tweets and granted us permission to use it in this chapter.

To comply with Twitter's terms of services, we will not provide any data from 
Twitter nor show any real tweets in this chapter. Instead, we can use Sanders'  
hand-labeled data, which contains the tweet IDs and their hand-labeled sentiment, 
and use his script, install.py, to fetch the corresponding Twitter data. As the  
script is playing nicely with Twitter's servers, it will take quite some time to 
download all the data for more than 5000 tweets. So it is a good idea to start  
it now.

The data comes with four sentiment labels:

We will treat irrelevant and neutral labels together and ignore all non-English  
tweets, resulting into 3642 tweets. These can be easily filtered using the data 
provided by Twitter.

Introducing the Naive Bayes classifier
Naive Bayes is probably one of the most elegant machine learning algorithms  
out there that is of practical use. Despite its name, it is not that naive when you  
look at its classification performance. It proves to be quite robust to irrelevant 
features, which it kindly ignores. It learns fast and predicts equally so. It does  
not require lots of storage. So, why is it then called naive?
The naive was added to the account for one assumption that is required for Bayes 
to work optimally: all features must be independent of each other. This, however, 
is rarely the case for real-world applications. Nevertheless, it still returns very good 
accuracy in practice even when the independent assumption does not hold.


Classification II – Sentiment AnalysisGetting to know the Bayes theorem
At its core, Naive Bayes classification is nothing more than keeping track of which 
feature gives evidence to which class. To ease our understanding, let us assume the 
following meanings for the variables that we will use to explain Naive Bayes:

Variable

Possible values
"pos", "neg"

Meaning
Class of a tweet (positive or negative)

Non-negative 
integers
Non-negative 
integers

Counting the occurrence of awesome in the tweet

Counting the occurrence of crazy in the tweet

During training, we learn the Naive Bayes model, which is the probability for a class 
 when we already know features 
.

. This probability is written as and Since we cannot estimate this directly, we apply a trick, which was found out  
by Bayes:

If we substitute of the probability for the data instance belonging to the specified class:
with the probability of both features as being our class , we arrive at the relationship that helps us to later retrieve and occurring and think This allows us to express by means of the other probabilities: We could also say that:


The prior and evidence values are easily determined:

is the prior probability of class 

without knowing about the data. This 

quantity can be obtained by simply calculating the fraction of all training 
data instances belonging to that particular class.

is the evidence, or the probability of features 

This can be 
retrieved by calculating the fraction of all training data instances having that 
particular feature value.
 

•  The tricky part is the calculation of the likelihood 

describing how likely it is to see feature values 
class of the data instance is 

. To estimate this we need a bit more thinking.

. It is the value 

 and 

 if we know that the 

Being naive
From the probability theory, we also know the following relationship:

This alone, however, does not help much, since we treat one difficult problem 
(estimating) with another one (estimating).

However, if we naively assume that are independent from each other, simplifies to and we can write it as follows:
Putting everything together, we get this quite manageable formula:

The interesting thing is that although it is not theoretically correct to simply tweak 
our assumptions when we are in the mood to do so, in this case it proves to work 
astonishingly well in real-world applications.

Classification II – Sentiment Analysis Using Naive Bayes to classify
Given a new tweet, the only part left is to simply calculate the probabilities:

We also need to choose the class 
classes the denominator, 
changing the winner class.

 having the higher probability. As for both 
, is the same, so we can simply ignore it without 

Note, however, that we don't calculate any real probabilities any more. Instead,  
we are estimating which class is more likely given the evidence. This is another 
reason why Naive Bayes is so robust: it is not so much interested in the real 
probabilities, but only in the information which class is more likely to. In short,  
we can write it as follows:

Here we are calculating the part after argmax for all classes of C ("pos" and "neg"  
in our case) and returning the class that results in the highest value.

But for the following example, let us stick to real probabilities and do some 
calculations to see how Naive Bayes works. For the sake of simplicity, we will 
assume that Twitter allows only for the two words mentioned earlier, awesome  
and crazy, and that we had already manually classified a handful of tweets:

In this case, we have six total tweets, out of which four are positive and two negative, 
which results in the following priors:

This means, without knowing anything about the tweet itself, we would be wise in 
assuming the tweet to be positive.

The piece that is still missing is the calculation of 
probabilities for the two features 

 and 

 conditioned on class C.

 and 

, which are the 

This is calculated as the number of tweets in which we have seen that the concrete 
feature is divided by the number of tweets that have been labeled with the class of 
. Let's say we want to know the probability of seeing awesome occurring once in a 
tweet knowing that its class is "positive"; we would have the following:

Since out of the four positive tweets three contained the word awesome, obviously 
the probability for not having awesome in a positive tweet is its inverse as we have 
seen only tweets with the counts 0 or 1:

Similarly for the rest (omitting the case that a word is not occurring in a tweet):

For the sake of completeness, we will also compute the evidence so that we can see 
 and 
real probabilities in the following example tweets. For two concrete values of 

,we can calculate the evidence as follows:


Classification II – Sentiment AnalysisThis denotation "" leads to the following values:

Now we have all the data to classify new tweets. The only work left is to parse the 
tweet and give features to it.


So far, so good. The classification of trivial tweets makes sense except for the last one, 
which results in a division by zero. How can we handle that?

Accounting for unseen words and  
other oddities
When we calculated the preceding probabilities, we actually cheated ourselves. We 
were not calculating the real probabilities, but only rough approximations by means of 
the fractions. We assumed that the training corpus would tell us the whole truth about 
the real probabilities. It did not. A corpus of only six tweets obviously cannot give us 
all the information about every tweet that has ever been written. For example, there 
certainly are tweets containing the word "text", it is just that we have never seen them. 
Apparently, our approximation is very rough, so we should account for that. This is 
often done in practice with "add-one smoothing".

Add-one smoothing is sometimes also referred to as additive smoothing 
or Laplace smoothing. Note that Laplace smoothing has nothing to do 
with Laplacian smoothing, which is related to smoothing of polygon 
meshes. If we do not smooth by one but by an adjustable parameter 
alpha greater than zero, it is called Lidstone smoothing.

It is a very simple technique, simply adding one to all counts. It has the underlying 
assumption that even if we have not seen a given word in the whole corpus, there is 
still a chance that our sample of tweets happened to not include that word. So, with 
add-one smoothing we pretend that we have seen every occurrence once more than 
we actually did. That means that instead of calculating the following:

We now calculate:

Why do we add 2 in the denominator? We have to make sure that the end result 
is again a probability. Therefore, we have to normalize the counts so that all 
probabilities sum up to one. As in our current dataset awesome, can occur either  
zero or one time, we have two cases. And indeed, we get 1 as the total probability:

Classification II – Sentiment Analysis Similarly, we do this for the prior probabilities:

Accounting for arithmetic underflows
There is yet another roadblock. In reality, we work with probabilities much smaller 
than the ones we have dealt with in the toy example. In reality, we also have more 
than two features, which we multiply with each other. This will quickly lead to the 
point where the accuracy provided by NumPy does not suffice anymore:

So, how probable is it that we will ever hit a number like 2.47E-324? To answer this, 
we just have to imagine a likelihood for the conditional probabilities of 0.0001 and 
then multiply 65 of them together (meaning that we have 65 low probable feature 
values) and you've been hit by the arithmetic underflow:

A float in Python is typically implemented using double in C. To find out whether 
it is the case for your platform, you can check it as follows:

To mitigate this, you could switch to math libraries such as mpmath (http://code.
google.com/p/mpmath/) that allow arbitrary accuracy. However, they are not fast 
enough to work as a NumPy replacement.

Chapter 6 Fortunately, there is a better way to take care of this, and it has to do with a nice 
relationship that we maybe still know from school:

If we apply it to our case, we get the following:

As the probabilities are in the interval between 0 and 1, the log of the probabilities 
lies in the interval -∞ and 0. Don't get irritated with that. Higher numbers are still a 
stronger indicator for the correct class—it is only that they are negative now.

There is one caveat though: we actually don't have log in the formula's nominator 
(the part preceding the fraction). We only have the product of the probabilities. In 
our case, luckily we are not interested in the actual value of the probabilities. We 
simply want to know which class has the highest posterior probability. We are lucky 
because if we find this:

Then we also have the following:

Classification II – Sentiment Analysis A quick look at the previous graph shows that the curve never goes down when we 
go from left to right. In short, applying the logarithm does not change the highest 
value. So, let us stick this into the formula we used earlier:

We will use this to retrieve the formula for two features that will give us the best 
class for real-world data that we will see in practice:

Of course, we will not be very successful with only two features, so let us rewrite it 
to allow the arbitrary number of features:

There we are, ready to use our first classifier from the scikit-learn toolkit.

Creating our first classifier and tuning it
The Naive Bayes classifiers reside in the sklearn.naive_bayes package. There are 
different kinds of Naive Bayes classifiers:

•  GaussianNB: This assumes the features to be normally distributed 

(Gaussian). One use case for it could be the classification of sex according 
to the given height and width of a person. In our case, we are given tweet 
texts from which we extract word counts. These are clearly not Gaussian 
distributed.

•  MultinomialNB: This assumes the features to be occurrence counts, which is 
relevant to us since we will be using word counts in the tweets as features. In 
practice, this classifier also works well with TF-IDF vectors.

•  BernoulliNB: This is similar to MultinomialNB, but more suited when using 

binary word occurrences and not word counts.

As we will mainly look at the word occurrences, for our purpose, MultinomialNB is 
best suited.

Chapter 6 Solving an easy problem first
As we have seen when we looked at our tweet data, the tweets are not just positive 
or negative. The majority of tweets actually do not contain any sentiment, but 
are neutral or irrelevant, containing, for instance, raw information (New book: 
Building Machine Learning ... http://link). This leads to four classes. To 
avoid complicating the task too much, let us for now only focus on the positive and 
negative tweets:

Now, we have in X the raw tweet texts and in Y the binary classification; we assign 0 
for negative and 1 for positive tweets.

As we have learned in the chapters before, we can construct TfidfVectorizer 
to convert the raw tweet text into the TF-IDF feature values, which we then use 
together with the labels to train our first classifier. For convenience, we will use the 
Pipeline class, which allows us to join the vectorizer and the classifier together and 
provides the same interface:

The Pipeline instance returned by create_ngram_model() can now be used for 
fit() and predict() as if we had a normal classifier.

Since we do not have that much data, we should do cross-validation. This time, 
however, we will not use KFold, which partitions the data in consecutive folds, but 
instead we use ShuffleSplit. This shuffles the data for us, but does not prevent the 
same data instance to be in multiple folds. For each fold, then, we keep track of the 
area under the Precision-Recall curve and the accuracy.

Classification II – Sentiment Analysis To keep our experimentation agile, let us wrap everything together in a train_
model() function, which takes a function as a parameter that creates the classifier:


Chapter 6 With our first try of using Naive Bayes on vectorized TF-IDF trigram features, we get 
an accuracy of 80.5 percent and a P/R AUC of 87.8 percent. Looking at the P/R chart 
shown in the following screenshot, it shows a much more encouraging behavior than 
the plots we saw in the previous chapter:

For the first time, the results are quite encouraging. They get even more impressive 
when we realize that 100 percent accuracy is probably never achievable in a 
sentiment classification task. For some tweets, even humans often do not really  
agree on the same classification label.

Using all the classes
But again, we simplified our task a bit, since we used only positive or negative 
tweets. That means we assumed a perfect classifier that classified upfront whether 
the tweet contains a sentiment and forwarded that to our Naive Bayes classifier.

So, how well do we perform if we also classify whether a tweet contains any 
sentiment at all? To find that out, let us first write a convenience function that  
returns a modified class array that provides a list of sentiments that we would like  
to interpret as positive


Note that we are talking about two different positives now. The sentiment of a tweet 
can be positive, which is to be distinguished from the class of the training data. If, for 
example, we want to find out how good we can separate the tweets having sentiment 
from neutral ones, we could do this as follows:


In Y we now have a 1 (positive class) for all tweets that are either positive or negative 
and a 0 (negative class) for neutral and irrelevant ones.

As expected, the P/R AUC drops considerably, being only 67 percent now. 
The accuracy is still high, but that is only due to the fact that we have a highly 
imbalanced dataset. Out of 3,642 total tweets, only 1,017 are either positive or 
negative, which is about 28 percent. This means that if we created a classifier  
that always classified a tweet as not containing any sentiments, we would already 
have an accuracy of 72 percent. This is another example of why you should always 
look at precision and recall if the training and test data is unbalanced.

Chapter 6 So, how would the Naive Bayes classifier perform on classifying positive tweets 
versus the rest and negative tweets versus the rest? One word: bad.

Pretty unusable if you ask me. Looking at the P/R curves shown in the following 
screenshots, we also find no usable precision/recall tradeoff as we were able to do  
in the previous chapter.

Tuning the classifier's parameters
Certainly, we have not explored the current setup enough and should  
investigate more. There are roughly two areas where we could play with the  
knobs: TfidfVectorizer and MultinomialNB. As we have no real intuition as  
to which area we should explore, let us try to distribute the parameters' values:

•  TfidfVectorizer

 ° Use different settings for NGrams: unigrams (1,1), bigrams (1,2),  

and trigrams (1,3)
Play with min_df: 1 or 2
Explore the impact of IDF within TF-IDF using use_idf and  
smooth_idf: False or True
Play with the idea of whether to remove stop words or not by  
setting stop_words to English or None

Classification II – Sentiment Analysis

Experiment with whether or not to use the logarithm of the word 
counts (sublinear_tf)
Experiment with whether or not to track word counts or simply track 
whether words occur or not by setting binary to True or False

•  MultinomialNB

 ° Decide which of the following smoothing methods to use by  

setting alpha:

 ° Add-one or Laplace smoothing: 1
 °
 ° No smoothing: 0

Lidstone smoothing: 0.01, 0.05, 0.1, or 0.5

A simple approach could be to train a classifier for all those reasonable exploration 
values while keeping the other parameters constant and checking the classifier's 
results. As we do not know whether those parameters affect each other, doing it 
right would require that we train a classifier for every possible combination of all 
parameter values. Obviously, this is too tedious for us to do.

Because this kind of parameter exploration occurs frequently in machine learning 
tasks, scikit-learn has a dedicated class for it called GridSearchCV. It takes an 
estimator (an instance with a classifier-like interface), which would be the pipeline 
instance in our case, and a dictionary of parameters with their potential values.

GridSearchCV expects the dictionary's keys to obey a certain format so that it is able 
to set the parameters of the correct estimator. The format is as follows:

Now, if we want to specify the desired values to explore for the min_df parameter of 
TfidfVectorizer (named vect in the Pipeline description), we would have to say:

Param_grid={"vect__ngram_range"=[(1, 1), (1, 2), (1, 3)]}

This would tell GridSearchCV to try out unigrams, bigrams, and trigrams as 
parameter values for the ngram_range parameter of TfidfVectorizer.

Then it trains the estimator with all possible parameter/value combinations. Finally, 
it provides the best estimator in the form of the member variable best_estimator_.

As we want to compare the returned best classifier with our current best one, we 
need to evaluate it the same way. Therefore, we can pass the ShuffleSplit instance 
using the CV parameter (this is the reason CV is present in GridSearchCV).


The only missing thing is to define how GridSearchCV should determine the best 
estimator. This can be done by providing the desired score function to (surprise!) the 
score_func parameter. We could either write one ourselves or pick one from the 
sklearn.metrics package. We should certainly not take metric.accuracy because 
of our class imbalance (we have a lot less tweets containing sentiment than neutral 
ones). Instead, we want to have good precision and recall on both the classes: the 
tweets with sentiment and the tweets without positive or negative opinions. One 
metric that combines both precision and recall is the F-measure metric, which is 
implemented as metrics.f1_score:

Putting everything together, we get the following code:

Classification II – Sentiment Analysis We have to be patient when executing the following code:

This is because we have just requested a parameter sweep over the 

 parameter combinations—each being trained on 10 folds:

The best estimator indeed improves the P/R AUC by nearly 3.3 percent to 70.2 with 
the setting that was printed earlier.

The devastating results for positive tweets against the rest and negative tweets 
against the rest will improve if we configure the vectorizer and classifier with those 
parameters that we have just found out:

== Pos vs. rest ==
0.883    0.005    0.520    0.028    
== Neg vs. rest ==
0.888    0.009    0.631    0.031        

Indeed, the P/R curves look much better (note that the graphs are from the medium 
of the fold classifiers, thus have slightly diverging AUC values):

Nevertheless, we probably still wouldn't use those classifiers. Time for something 
completely different!

Cleaning tweets
New constraints lead to new forms. Twitter is no exception in this regard. Because 
text has to fit into 140 characters, people naturally develop new language shortcuts 
to say the same in less characters. So far, we have ignored all the diverse emoticons 
and abbreviations. Let's see how much we can improve by taking that into 
account. For this endeavor, we will have to provide our own preprocessor() to 
TfidfVectorizer.

Classification II – Sentiment AnalysisFirst, we define a range of frequent emoticons and their replacements in a dictionary. 
Although we could find more distinct replacements, we go with obvious positive or 
negative words to help the classifier:

# make sure that e.g. :dd is replaced before :d
emo_repl_order = [k for (k_len,k) in reversed(sorted([(len(k),k) for k 
in emo_repl.keys()]))]

Then, we define abbreviations as regular expressions together with their expansions 

Certainly, there are many more abbreviations that could be used here. But already 
with this limited set, we get an improvement for sentiment versus not sentiment of 
half a point, which comes to 70.7 percent:

Taking the word types into account
So far our hope was to simply use the words independent of each other with the 
hope that a bag-of-words approach would suffice. Just from our intuition, however, 
neutral tweets probably contain a higher fraction of nouns, while positive or negative 
tweets are more colorful, requiring more adjectives and verbs. What if we could 
use this linguistic information of the tweets as well? If we could find out how many 
words in a tweet were nouns, verbs, adjectives, and so on, the classifier could maybe 
take that into account as well.

Classification II – Sentiment Analysis Determining the word types
Determining the word types is what part of speech (POS) tagging is all about. A 
POS tagger parses a full sentence with the goal to arrange it into a dependence tree, 
where each node corresponds to a word and the parent-child relationship determines 
which word it depends on. With this tree, it can then make more informed decisions; 
for example, whether the word "book" is a noun ("This is a good book.") or a verb 
("Could you please book the flight?").

You might have already guessed that NLTK will also play a role also in this area. 
And indeed, it comes readily packaged with all sorts of parsers and taggers. The POS 
tagger we will use, nltk.pos_tag(), is actually a full-blown classifier trained using 
manually annotated sentences from the Penn Treebank Project (http://www.cis.
upenn.edu/~treebank). It takes as input a list of word tokens and outputs a list of 
tuples, each element of which contains the part of the original sentence and its part of 
speech tag:
Description
coordinating conjunction
cardinal number
determiner
existential there
foreign word
preposition/subordinating 
conjunction
adjective
adjective, comparative
adjective, superlative
list marker
modal


Description
noun, singular or mass
noun plural
proper noun, singular
proper noun, plural
predeterminer
possessive ending
personal pronoun
possessive pronoun

adverb

adverb, comparative
adverb, superlative
particle
to
interjection
verb, base form
verb, past tense
verb, gerund/present participle
verb, past participle
verb, singular, present, non-3D
verb, third person singular, present
wh-determiner
wh-pronoun
possessive wh-pronoun
wh-abverb

Example
book
books
Sean
Vikings
both the boys
friend's
I, he, it
my, his
however, usually, 
naturally, here, good
better
best
give up
to go, to him
uhhuhhuhh
take
took
taking
taken
take
takes
which
who, what
whose
where, when

With these tags it is pretty easy to filter the desired tags from the output of pos_
tag(). We simply have to count all the words whose tags start with NN for nouns, VB 
for verbs, JJ for adjectives, and RB for adverbs.

Classification II – Sentiment Analysis Successfully cheating using SentiWordNet
While the linguistic information that we discussed earlier will most likely help 
us, there is something better we can do to harvest it: SentiWordNet (http://
sentiwordnet.isti.cnr.it). Simply put, it is a 13 MB file that assigns most of 
the English words a positive and negative value. In more complicated words, for 
every synonym set, it records both the positive and negative sentiment values. Some 
examples are as follows:

attention or consideration 
or forethought or 
thoroughness; not careful
A prosthesis placed 
permanently in tissue
Form a curl, curve, or 
kink; "the cigar smoke 
curled up at the ceiling"

With the information in the POS column, we will be able to distinguish between the 
noun "book" and the verb "book". PosScore and NegScore together will help us to 
determine the neutrality of the word, which is 1-PosScore-NegScore. SynsetTerms 
lists all words in the set that are synonyms. The ID and Description can be safely 
ignored for our purpose.

The synset terms have a number appended, because some occur multiple times in 
different synsets. For example, "fantasize" conveys two quite different meanings, also 
leading to different scores:

Description
Portray in the mind; "he is 
fantasizing the ideal wife"
Indulge in fantasies; "he is 
fantasizing when he says 
that he plans to start his 
own company"

To find out which of the synsets to take, we would have to really understand the 
meaning of the tweets, which is beyond the scope of this chapter. The field of 
research that focuses on this challenge is called word sense disambiguation. For 
our task, we take the easy route and simply average the scores over all the synsets 
in which a term is found. For "fantasize", PosScore would be 0.1875 and NegScore 
would be 0.0625.

The following function, load_sent_word_net(), does all that for us, and returns a 
dictionary where the keys are strings of the form "word type/word", for example "n/
implant", and the values are the positive and negative scores:

Classification II – Sentiment AnalysisOur first estimator
Now we have everything in place to create our first vectorizer. The most convenient 
way to do it is to inherit it from BaseEstimator. It requires us to implement the 
following three methods:

•  get_feature_names(): This returns a list of strings of the features that we 

will return in transform().

•  fit(document, y=None): As we are not implementing a classifier, we can 

ignore this one and simply return self.

•  transform(documents): This returns numpy.array(), containing an array of 

shape (len(documents), len(get_feature_names)). This means that for 
every document in documents, it has to return a value for every feature name 
in get_feature_names().

Let us now implement these methods:

Putting everything together
Nevertheless, using these linguistic features in isolation without the words themselves 
will not take us very far. Therefore, we have to combine TfidfVectorizer with 
the linguistic features. This can be done with scikit-learn's FeatureUnion class. It 
is initialized the same way as Pipeline, but instead of evaluating the estimators 
in a sequence and each passing the output of the previous one to the next one, 
FeatureUnion does it in parallel and joins the output vectors afterwards:


Training and testing on the combined featurizers gives another 0.6 percent 
improvement on positive versus negative:

== Pos vs. neg ==
0.808    0.016    0.892    0.010    
== Pos/neg vs. irrelevant/neutral ==
0.794    0.009    0.707    0.033    
== Pos vs. rest ==
0.886    0.006    0.533    0.026    
== Neg vs. rest ==
0.881    0.012    0.629    0.037

With these results, we probably do not want to use the positive versus rest and 
negative versus rest classifiers, but instead use first the classifier determining 
whether the tweet contains sentiment at all ("pos/neg versus irrelevant/neutral") 
and then, when it does, use the positive versus negative classifier to determine the 
actual sentiment.

Summary
Congratulations for sticking with us until the end! Together we have learned how 
Naive Bayes work and why they are not that naive at all. For training sets where we 
don't have enough data to learn all the niches in the class probability space, Naive 
Bayes do a great job of generalizing. We learned how to apply them to tweets and 
that cleaning the rough tweets' text helps a lot. Finally, we realized that a bit of 
"cheating" (only after we have done our fair share of work) is OK, especially, when it 
gives another improvement of the classifier's performance, as we have experienced 
with the use of SentiWordNet.
